---
layout: page
title: Invited Talks
---

# Invited Talks


<a name='shi'></a>
## An Implementation of Web Application Firewall Based on a Deep Nerual Network Detection Engine.
Most enterprises provide Web services open to the public and thus are prone to Web attacks. Among these attacks, SQL Injection (SQLi) is a very serious threat, which could lead to catastrophic data leaking and loss. Lots of research efforts have been devoted into SQLi detection areas thereafter and heuristic rule based Web Application Firewall (WAF) was invented and deployed for protections. However, subtle and carefully crafted SQLi attacks can still easily hide themselves from being detected. Deep learning methods were introduced in recent years and they have shown a great strength to learn and describe complex semantic contents. In this talk, we present a CNN based SQLi detection implementation, which also has an ability to propose locations of suspect attack payloads within an URL request. Experimental results show that our implementation outperforms a leading method called Libinjection and our current version Alibaba Cloud WAF.

*Liang Shi, Tianlong Liu, Min Ye*


<a name='reza'></a>
##Data Privacy in Machine Learning

I will talk about what machine learning privacy is, and will discuss
how and why machine learning models leak information about the
individual data records on which they were trained. My quantitative
analysis will be based on the fundamental membership inference
attacks: given a data record and (black-box) access to a model,
determine if a record was in the model's training set. I will
demonstrate how to build such inference attacks on different
classification models e.g., trained by commercial "machine learning
as a service" providers such as Google and Amazon

*Reza Shokri*


<a name='ian'></a>
##Adversarial Deep Learning: Attacks and Defenses

Deep Learning models are vulnerable to adversarial attacks, which can reliably cause the models to misbehave, for example convincing an image classifier that an image of a cat is actually a dog.  We will discuss some of the recent attacks and attempts at defending against such attacks, and look at how adversarial attacks may be harnessed to improve the robustness of Deep Learning models.

*Ian Fischer*



